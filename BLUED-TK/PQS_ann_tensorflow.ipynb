{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/media/sf_datasets/Smarter Devices/BLUED_extracted/BLUED-TK\n"
     ]
    }
   ],
   "source": [
    "cd /media/sf_datasets/Smarter\\ Devices/BLUED_extracted/BLUED-TK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "\n",
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#reading the dataframe\n",
    "df=pd.read_table('events/peaks.csv',index_col=0,sep='\\t')\n",
    "\n",
    "#converting the string to numpy array\n",
    "def makeArray(text):\n",
    "    text=text.replace(\"[\",\" \").strip()\n",
    "    return np.fromstring(text,sep=' ')\n",
    "\n",
    "df.features=df.features.apply(makeArray)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#finding the single event devices\n",
    "freq=df.groupby('label').count()\n",
    "# freq=freq.sort_values(by='features')\n",
    "single_freq=freq[freq.features==1].index\n",
    "# single_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#eliminating all the single event devices\n",
    "for i in single_freq:\n",
    "    df=(df[df['label']!=i])\n",
    "# #eliminating refrigerator\n",
    "# df=(df[df['label']!=111])\n",
    "# #eliminating monitor\n",
    "# df=(df[df['label']!=140])\n",
    "# print(len(df))\n",
    "# printing the values by count\n",
    "# df.groupby('label').count().sort_values(by='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# #what's causing the problem\n",
    "# for i in df.label.unique():\n",
    "#     print(i)\n",
    "#     val=df[df.label==i].features\n",
    "#     for v in val:\n",
    "#         plt.plot(v)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputs=70\n",
    "layer1=100\n",
    "layer2=100\n",
    "outputs=34\n",
    "\n",
    "total_events=len(df)\n",
    "\n",
    "x = tf.placeholder(tf.float32, shape=[None, inputs])\n",
    "y = tf.placeholder(tf.float32, shape=[None, outputs])\n",
    "\n",
    "\n",
    "W = tf.Variable(tf.zeros([inputs,outputs]))\n",
    "b = tf.Variable(tf.zeros([outputs]))\n",
    "\n",
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([inputs, layer1])),\n",
    "    'h2' :tf.Variable(tf.random_normal([layer1, layer2])),\n",
    "    'ho' :tf.Variable(tf.random_normal([layer2, outputs])),    \n",
    "}\n",
    "\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([layer1])),\n",
    "    'b2': tf.Variable(tf.random_normal([layer2])),\n",
    "    'bo': tf.Variable(tf.random_normal([outputs])),\n",
    "\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#output layer\n",
    "def ann(x,weights,biases):\n",
    "    layer1=tf.add(tf.matmul(x,weights['h1']),biases['b1'])\n",
    "    layer2=tf.add(tf.matmul(layer1, weights['h2']), biases['b2'])\n",
    "    out_layer = tf.add(tf.matmul(layer2, weights['ho']), biases['bo'])\n",
    "    return out_layer\n",
    "    \n",
    "pred = ann(x, weights, biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2226, 70)\n",
      "(2226, 34)\n"
     ]
    }
   ],
   "source": [
    "feature_matrix=np.empty((total_events,inputs))\n",
    "label_matrix=np.zeros((total_events,outputs))\n",
    "\n",
    "ll=np.empty((total_events))\n",
    "count=0\n",
    "for i in df.index:\n",
    "        feature_matrix[count]=df.loc[i].features\n",
    "        ll[count]=df.loc[i].label\n",
    "        count+=1\n",
    "        \n",
    "device_list=list(df.label.unique())\n",
    "for i in range(0,len(ll)):\n",
    "    label_matrix[i,device_list.index(ll[i])]=1\n",
    "print(feature_matrix.shape)\n",
    "print(label_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  4.65933250e-01   5.29614250e-01   5.93295250e-01 ...,  -1.27584998e+01\n",
      "   -1.27160457e+01  -1.26735917e+01]\n",
      " [ -6.11480500e-02  -8.22340500e-02   1.07536950e-01 ...,   4.02736950e-01\n",
      "    4.44907950e-01   1.91879950e-01]\n",
      " [  2.11209250e-01   4.44706250e-01   5.08387250e-01 ...,  -1.27584998e+01\n",
      "   -1.27797268e+01  -1.27372728e+01]\n",
      " ..., \n",
      " [ -1.16748500e-01  -7.42945000e-02  -1.37975500e-01 ...,  -5.30675000e-02\n",
      "   -1.06135000e-02  -1.06135000e-02]\n",
      " [ -3.50245500e-02   1.56018450e-01   1.34791450e-01 ...,   1.77245450e-01\n",
      "    2.86564500e-02   9.23374500e-02]\n",
      " [  7.48539000e-02   9.59399000e-02   1.15969000e-02 ...,   2.43539900e-01\n",
      "   -3.05741000e-02   3.26829000e-02]]\n"
     ]
    }
   ],
   "source": [
    "print(feature_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.003\n",
    "training_epochs = 1000\n",
    "display_step = training_epochs/10\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=pred, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "# Initializing the variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "# Launch the graph\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(training_epochs):\n",
    "        avg_cost = 0.\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: feature_matrix,\n",
    "                                                      y: label_matrix})\n",
    "        avg_cost = c \n",
    "        correct_prediction = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "        # Calculate accuracy\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "        if not epoch%display_step:\n",
    "            print (\"Accuracy:%f\" %(accuracy.eval({x: feature_matrix, y: label_matrix})*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
